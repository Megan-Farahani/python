# -*- coding: utf-8 -*-
"""assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HHKAaBNql0SXjFuPeMldq0mhJKQ3m6tw

# **Question 1 (20)**
# Dataset: Life_Expectancy_Data.csv **bold text**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""1. Load the dataset. (1)"""

from google.colab import files #command for uploading dataset to google colab
upload = files.upload()

"""2. Display the first 20 rows. (1)"""

df = pd.read_csv ("Life_Expectancy_Data.csv") # eading the dataset
df.head(20) #showing the first 20 rows starting from 0 to 19

"""*Understanding the dateset*"""

df.describe().round(2)

"""3. Find the number of null values in the dataset. (1)"""

# Find null rate for each column
null_rate = {}
size = len(df)
for column in df:
  null_rate[column] = df[column].isna().sum()*100/size

#print null eate for each column
null_rate

# Remove null values
new_df = df.dropna()

# Check null values are removed
new_df.isna().sum()

null_count = df.isnull().sum() #the number of null values in each colum
total_null_values = null_count.sum() # the total number of null values in the entire dataset
print("Total number of null values:", total_null_values )

"""4. Impute the missing values with the mean values of the data. You can use SimpleImputer
from sklearn.impute. (1)

"""

#from sklearn.impute import SimpleImputer
from sklearn.impute import SimpleImputer

# Initialize SimpleImputer with strategy='mean'
imputer = SimpleImputer(strategy='mean')

# Extract numeric columns
numeric_columns = df.select_dtypes(include=np.number).columns

# Impute missing values with the mean of each column
df[numeric_columns] = imputer.fit_transform(df[numeric_columns])

# Check if missing values are filled
df.head(50)
# Check if missing value in the row index 32 is filled. This row is visible
df.iloc[32] # we see in the column Alcohol it used to be NaN and now it is filled by the mean

"""Instead of imputing the missing values with the mean value of the whole column,
impute it with the mean value of the column that corresponds to the country.
"""

from sklearn.impute import SimpleImputer

# Exclude non-numeric columns from grouping
numeric_columns = df.select_dtypes(include=np.number).columns

# Group the DataFrame by the 'Country' column and calculate the mean for each group
grouped_by_country = df.groupby('Country')[numeric_columns].mean()

# Initialize SimpleImputer with strategy='mean'
imputer = SimpleImputer(strategy='mean')

# Apply the imputer to each group
df_imputed = grouped_by_country.transform(lambda x: x.fillna(x.mean()))

# Now, apply the imputer to the entire DataFrame to handle any remaining missing values
df_imputed[numeric_columns] = imputer.fit_transform(df_imputed[numeric_columns])

#grouped_by_country.mean()
df_imputed.isnull().sum() # It shows that the number of missing values are zero in each column

"""5. Find the count, mean, standard deviation, quartiles and extrema for the numeric
columns. (1)
"""

# Find all of the above statistics using only one code
new_df.describe().round(2)

"""Visualization (15)
6. Find the correlation between the numeric columns and display your findings on a
heatmap. (2)
"""

#Understanding what data type we have before visualization
df.dtypes
df.info()

import seaborn as sns
import matplotlib.pyplot as plt

# Calculate correlation matrix
correlation_matrix = new_df.corr(numeric_only=True)

# Plot heatmap
plt.figure(figsize=(12, 12))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
plt.title("Correlation Between Numeric Columns")
plt.show()

"""This plot visualizes the correlation between variables, ranging from -1 to +1. Values closer to zero indicate no linear trend, while values closer to 1 or -1 show strong positive ( that is, as one increases,
 so does the other )or negative correlations, respectively. The diagonals are always 1 (dark red) as they represent the correlation of each variable with itself. The plot is symmetrical around the diagonal, pairing the same two variables together, and darker colours indicate higher correlations.

7. Plot a histogram of the life expectancy. (2)
"""

# Size of graph
plt.figure(figsize=(12, 6))
# Plot histogram
#plt.hist(new_df['Life expectancy '], bins=[44,50,55,60,65,70,75,80,85,89], color='skyblue', edgecolor='black')
plt.hist(new_df['Life expectancy '], bins=9 , color='skyblue', edgecolor='black') #bins= (max-min)/5 = (89-44)/5 = 45/5 = 9

# Labele x
plt.xlabel('Life Expectancy')
# Labele y
plt.ylabel('Frequency')
# Labele the whole graph
plt.title('Histogram of Life Expectancy')
# Use grid as a background
#plt.grid(True)
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)
# print the graph
plt.show()

"""The plot shows the relationship between Life Expectancy and Frequency of  Life Expectancy. The frequency of life expectancy rises gradually from approximately 44 years to 65, then doubles at 65 to peak at 70. The most frequent is assigned to the ages 70 to 75 years. Subsequently, it declines to about 300 for the next five years. Then, there is a significant decrease in the frequency of life expectancy for individuals aged 80 to 85, one in three compared to five years earlier. Additionally, the graph shows the fewest people over the entire graph in the age group of 85 to 89.

8. Compare the life expectancy in developed countries to that in developing countries using
violin plots next to each other. (2)
"""

# Create a violin plot comparing life expectancy in developed and developing countries
plt.figure(figsize=(10, 6))
sns.violinplot(x='Life expectancy ', y='Status', hue='Status', data=new_df, palette='Set2', legend=False)

plt.title('Comparison of Life Expectancy in Developed and Developing Countries')
plt.xlabel('Life expectancy ')
plt.ylabel('Status')
plt.show()

"""This violin plot shows the relationship between life expectancy (the continuous variable) and the status of developing and developed countries.  The box plot elements show that the median status for developing countries is lower than for developed ones. That means we expect more life expectancy in the developed countries than in the developing ones. The shape of the distribution (extremely skinny on each end and wide in the middle) indicates that the status of developing countries is highly concentrated around the median. Also, Developed countries represent a more uniform or symmetric plot, meaning that the data distribution is uniform in these countries compared to the developing countries.

9. On the same line plot, display the life expectancy from 2000 to 2015 for Canada, the
United Kingdom and the United States of America. (2)
"""

# Filter the DataFrame for Canada, the United Kingdom, and the United States of America
countries = ['Canada', 'United Kingdom of Great Britain and Northern Ireland', 'United States of America']
filtered_df = df[df['Country'].isin(countries)]

# Plot the life expectancy for each country from 2000 to 2015
plt.figure(figsize=(8, 6))
for country in countries:
    country_data = filtered_df[filtered_df['Country'] == country]
    plt.plot(country_data['Year'], country_data['Life expectancy '], label=country)

plt.title('Life Expectancy from 2000 to 2015')
plt.xlabel('Year')
plt.ylabel('Life expectancy ')
plt.legend()
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)
plt.show()

"""The plot illustrates the relationship between the year and life expectancy in Canada, the United Kingdom, and the USA. It reveals a steady increase in life expectancy in the USA over the given years, starting at just under 78 years and reaching nearly 80 years. Conversely, in Canada, life expectancy was highest in 2000 compared to the other countries, hovering just under 80 years, while it was under 78 in the USA during the same period. In Canada, there was a notable spike in life expectancy from just under 80 to over 86 in 2008, followed by an unexpected drop to 81 the following year, and a gradual recovery thereafter.
For the United Kingdom of Great Britain and Northern Ireland, the pattern of life expectancy closely resembles that of Canada. However, there is a slight Delay in the increase in life expectancy compared to Canada. The length of life in the United Kingdom began to increase notably in 2008, four years later than in Canada. Also the same was true for the United Kingdom's life expectancy, which peaked in 2013.

10. Compare the average infant deaths over the years against the average life expectancy
over the years using a scatter plot for the following countries: Belgium, Brazil,
Cameroon, Canada, China, France, Ghana, India, the United Kingdom and the United
States of America. What can you conclude? (2)
"""

# Define the list of countries of interest
countries_of_interest = ['Belgium', 'Brazil', 'Cameroon', 'Canada', 'China',
                         'France', 'Ghana', 'India', 'United Kingdom', 'United States of America']

# Filter the DataFrame for the specified countries
filtered_df = df[df['Country'].isin(countries_of_interest)]

# Calculate average infant deaths and average life expectancy for each year
average_infant_deaths = filtered_df.groupby('Year')['infant deaths'].mean()
average_life_expectancy = filtered_df.groupby('Year')['Life expectancy '].mean()

# Create scatter plot
plt.figure(figsize=(12, 10))
#plt.scatter(average_infant_deaths, average_life_expectancy, marker='o', color='blue')
plt.scatter(average_infant_deaths, average_life_expectancy, color='blue')
# Add labels for each point
for year in average_infant_deaths.index:
    plt.text(average_infant_deaths[year], average_life_expectancy[year], str(year))

# Fit a linear regression line
slope, intercept = np.polyfit(average_infant_deaths, average_life_expectancy, 1)
# Plot regression line
plt.plot(average_infant_deaths, slope * average_infant_deaths + intercept, color='red')


# Customize plot
plt.title('Average Infant Deaths vs. Average Life Expectancy')
plt.xlabel('Average Infant Deaths')
plt.ylabel('Average Life Expectancy')
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)

# Show plot
plt.show()

"""The figure shows a relationship between average life expectancy and average infant deaths. The figure suggests a trend of decreasing infant deaths and increasing life expectancy over the given years (from 2000 to 2015), with the most recent data showing lower infant mortality rates and higher life expectancies in 2015 compared to the earlier two years, 2013 and 2014. Also, Most years are closely aligned with a regression line, indicating a robust relationship between the variables. However, 2006 to 2008 appears to deviate from this trend. So,  additional investigation may be necessary to understand the underlying reasons for this trend. In 2014, there was the highest life expectancy, indicating a positive trend in life expectancy over time. In 2015, there was the lowest average of infant deaths, suggesting improvements in infant mortality rates. Conversely, the highest number of infant deaths was recorded in 2000. These data points provide historical context and highlight trends in life expectancy and infant mortality rates over time.

11. In the year 2012
a. Compare the life expectancy with schooling using a scatter plot. (2)
b. What is the Pearson correlation? (1)
c. Draw the best regression line on the same plot as (a). (1)
d. What can you conclude? (1)
"""

from scipy.stats import pearsonr

# Filter data for the year 2012
data_2012 = df[df['Year'] == 2012]

# Remove rows with NaN values in the 'Life expectancy' or 'Schooling' columns because I get errors for NaN values
data_2012 = data_2012.dropna(subset=['Life expectancy ', 'Schooling'])

# Create a scatter plot comparing life expectancy with schooling
sns.scatterplot(x='Life expectancy ', y='Schooling', data=data_2012)

# Draw the best-fit regression line
sns.regplot(x='Life expectancy ', y='Schooling', data=data_2012, scatter=False, color='red')

# Calculate Pearson correlation coefficient
correlation, _ = pearsonr(data_2012['Life expectancy '], data_2012['Schooling'])
print("Pearson Correlation Coefficient:", correlation)

plt.title('Scatter Plot of Life Expectancy vs Schooling (Year 2012)')
plt.xlabel('Life Expectancy ')
plt.ylabel('Schooling')
plt.grid(color = 'green', linestyle = '--', linewidth = 0.5)
plt.show()

"""The graph shows the relationship between Life Expectancy and Schooling. As for the graph, the positive sign in the coefficient value indicates that life expectancy also tends to increase as schooling increases.
The coefficient value of 0.799, rounded above 0.80, suggests a strong positive correlation, meaning there is a high tendency for life expectancy to increase as schooling increases.
This implies a significant association between the level of education (measured by schooling) and life expectancy.This implies that as people receive more education (measured by years of schooling), their life expectancy tends to increase.

# **Question 2 (20)**
# Dataset: text.csv **bold text**
Preprocessing (2)
1. Load the dataset. (1)
2. Display the first 5 rows. (1)
"""

from google.colab import files #command for uploading dataset to google colab
upload = files.upload()

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import string

df = pd.read_csv ("text.csv") # eading the dataset
df.head(5) #showing the first 5 rows starting from 0 to 4

"""Cleaning (8) - You might want to create a new column that will store the cleaned text.
After each part (3-6), display the first 5 rows to check if you cleaned the data correctly.
3. Convert the text in the review column to lowercase.(1)
4. Remove stopwords. (2)
5. Remove punctuation signs. (2)
6. Apply lemmatization to every word in the cleaned column. (2)
"""

import string
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import nltk

nltk.download('wordnet')
nltk.download('stopwords')

# Initialize the WordNetLemmatizer
lemmatizer = WordNetLemmatizer()

stop_words = set(stopwords.words('english'))

def cleanTweets(text):
    if isinstance(text, str):  # Check if the value is a string (not NaN)
        text = text.lower()  # Convert text to lowercase (question 3)
        text = " ".join([word for word in text.split() if word not in stop_words])  # Remove stopwords (question 4)
        text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation signs (question 5)
        lemmatized_text = ' '.join([lemmatizer.lemmatize(word) for word in text.split()])  # Lemmatize each word individually (question 6)
    return lemmatized_text  # Return the cleaned text

# Remove rows with missing values (question 7)
df.dropna(inplace=True)

# Apply the cleanTweets function to the 'review' column
df['clean_text'] = df['review'].apply(cleanTweets)

# Display the modified DataFrame to check everything is cleaned
df.head(5)

"""In the table above, uppercase words have been converted to lowercase in the clean_text column. Additionally, there are no punctuation marks or stop words present. Furthermore, lemmatization has been applied, such as the word "tries" being converted to its root form "try" in row 0.

TF-IDF (10)
8. Create 2 new dataframes – one that contains reviews with positive sentiment (where the
label = “pos”) and one with negative sentiment (where the label = “neg”). Note that we
will be working with the cleaned text. (4)
9. Calculate the TF-IDF for positive cleaned reviews and for negative cleaned reviews. (4)
10. What are the 10 most important words in each dataset? (2)
"""

# Create a new DataFrame containing reviews with positive sentiment and cleaned text
positive_reviews_df = df[df['label'] == 'pos'][['clean_text']]
positive_reviews_df['sentiment'] = 'pos'  # Assign 'pos' to the sentiment column for positive reviews

# Import the TF-IDF vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the positive cleaned reviews
positive_tfidf = tfidf_vectorizer.fit_transform(positive_reviews_df['clean_text'])

# Get feature names (words)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Get TF-IDF values for positive reviews
positive_tfidf_values = positive_tfidf.toarray()

# Get index of maximum TF-IDF value for each review
positive_max_indices = positive_tfidf_values.argmax(axis=1)

# Get the 10 most important words for positive reviews
positive_top_words = [feature_names[index] for index in positive_max_indices if index < len(feature_names)]

# Display the first 10 rows of the DataFrame
positive_reviews_df.head(10)

# Create a new DataFrame containing reviews with negative sentiment and cleaned text
negative_reviews_df = df[df['label'] == 'neg'][['clean_text']]
negative_reviews_df['sentiment'] = 'neg'  # Assign 'neg' to the sentiment column for negative reviews

# Import the TF-IDF vectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize the TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit and transform the negative cleaned reviews
negative_tfidf = tfidf_vectorizer.fit_transform(negative_reviews_df['clean_text'])

# Get feature names (words)
feature_names = tfidf_vectorizer.get_feature_names_out()

# Get TF-IDF values for negative reviews
negative_tfidf_values = negative_tfidf.toarray()

# Get index of maximum TF-IDF value for each review
negative_max_indices = negative_tfidf_values.argmax(axis=1)

# Get the 10 most important words for negative reviews
negative_top_words = [feature_names[index] for index in negative_max_indices if index < len(feature_names)]

# Display the first 10 rows of the DataFrame
negative_reviews_df.head(10)